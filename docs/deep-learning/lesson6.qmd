---
title: "6: Random forests"
---

::: {layout="[30,70]"}

![](images/forest.png)

Random forests started a revolution in machine learning 20 years ago. For the first time, there was a fast and reliable algorithm which made almost no assumptions about the form of the data, and required almost no preprocessing. In today's lesson, you'll learn how a random forest really works, and how to build one from scratch. And, just as importantly, you'll learn how to interpret random forests to better understand your data.

:::

## Video

<iframe width="514" height="289" src="https://www.youtube-nocookie.com/embed/AdhG64NF76E?modestbranding=1" title="fast.ai lesson 6" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

This lesson is based partly on [chapter 9](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb) of the [book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527).

## Lesson notebooks

- [ How random forests really work](https://www.kaggle.com/code/jhoward/how-random-forests-really-work/)
- [Road to the top, part 1](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1)

## Links from the lesson

- [How to explain Gradient Boosting](https://explained.ai/gradient-boosting/)
- ["Statistical Modeling: The Two Cultures"](https://www.semanticscholar.org/paper/Statistical-modeling%3A-The-two-cultures-Breiman/e5df6bc6da5653ad98e754b08f63326c2e52b372)  by Leo Breiman

